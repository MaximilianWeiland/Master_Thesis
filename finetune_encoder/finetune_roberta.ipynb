{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (0.22.1)\n",
      "Requirement already satisfied: filelock in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maxweiland/Desktop/SEDS/Master_Thesis/venv_thesis/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch torchvision\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForTokenClassification\n",
    "import torch, torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# function that creates BIO-tags for text\n",
    "def tokenize_and_align_labels(text, entities, label_to_id):\n",
    "    # Tokenize and get offsets\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, truncation=True)\n",
    "    labels = [\"O\"] * len(encoding.offset_mapping)\n",
    "    \n",
    "    for ent in entities:\n",
    "        start, end = ent[\"start\"], ent[\"end\"]\n",
    "        ent_label = ent[\"labels\"][0]  # assume one label per span\n",
    "        \n",
    "        for idx, (token_start, token_end) in enumerate(encoding.offset_mapping):\n",
    "            if token_start >= end or token_end <= start:\n",
    "                continue\n",
    "            if token_start >= start and token_end <= end:\n",
    "                labels[idx] = f\"I-{ent_label}\"\n",
    "        \n",
    "        # Change first matched token to B-\n",
    "        for idx, label in enumerate(labels):\n",
    "            if label == f\"I-{ent_label}\":\n",
    "                labels[idx] = f\"B-{ent_label}\"\n",
    "                break\n",
    "\n",
    "    # Convert labels to IDs\n",
    "    label_ids = [label_to_id.get(label, label_to_id[\"O\"]) for label in labels]\n",
    "    return encoding[\"input_ids\"], encoding[\"attention_mask\"], label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Label Studio JSON\n",
    "with open(\"../data/annotations.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build label set dynamically\n",
    "label_set = {\"O\"}\n",
    "for task in data:\n",
    "    for result in task[\"annotations\"][0][\"result\"]:\n",
    "        if result[\"type\"] == \"labels\":\n",
    "            for lbl in result[\"value\"][\"labels\"]:\n",
    "                label_set.add(f\"B-{lbl}\")\n",
    "                label_set.add(f\"I-{lbl}\")\n",
    "label_list = sorted(label_set)\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for task in data:\n",
    "    text = task[\"data\"][\"sentence\"]\n",
    "    results = task[\"annotations\"][0][\"result\"]\n",
    "    spans = [\n",
    "        {\n",
    "            \"start\": r[\"value\"][\"start\"],\n",
    "            \"end\": r[\"value\"][\"end\"],\n",
    "            \"labels\": r[\"value\"][\"labels\"]\n",
    "        }\n",
    "        for r in results if r[\"type\"] == \"labels\"\n",
    "    ]\n",
    "    input_ids, attention_mask, label_ids = tokenize_and_align_labels(text, spans, label_to_id)\n",
    "    dataset.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": label_ids,\n",
    "        \"text\": text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = []\n",
    "not_include = []\n",
    "for sentence in dataset:\n",
    "    labels = sentence[\"labels\"]\n",
    "    if 0 in labels:\n",
    "        include.append(sentence)\n",
    "    else:\n",
    "        not_include.append(sentence)\n",
    "\n",
    "split_idx = int(len(include) * 0.75)\n",
    "train_include = include[:split_idx]\n",
    "test_include = include[split_idx:]\n",
    "\n",
    "train_dataset = train_include + random.sample(not_include, int(len(not_include) * 0.3))\n",
    "\n",
    "test_dataset = test_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract lists\n",
    "input_ids_list = [item[\"input_ids\"] for item in train_dataset]\n",
    "attention_masks_list = [item[\"attention_mask\"] for item in train_dataset]\n",
    "label_ids_list = [item[\"labels\"] for item in train_dataset]\n",
    "\n",
    "# Step 2: Compute max length\n",
    "max_len = max(len(seq) for seq in input_ids_list)\n",
    "\n",
    "# Step 3: Pad all sequences\n",
    "def pad(seq, pad_val, max_len):\n",
    "    return seq + [pad_val] * (max_len - len(seq))\n",
    "\n",
    "input_ids = torch.tensor([pad(seq, pad_val=1, max_len=max_len) for seq in input_ids_list])           # pad_val=1 for RoBERTa\n",
    "attention_masks = torch.tensor([pad(seq, pad_val=0, max_len=max_len) for seq in attention_masks_list])\n",
    "labels = torch.tensor([pad(seq, pad_val=-100, max_len=max_len) for seq in label_ids_list])           # -100 for ignored loss\n",
    "\n",
    "# Step 4: Wrap into TensorDataset\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract lists\n",
    "input_ids_list = [item[\"input_ids\"] for item in test_dataset]\n",
    "attention_masks_list = [item[\"attention_mask\"] for item in test_dataset]\n",
    "label_ids_list = [item[\"labels\"] for item in test_dataset]\n",
    "\n",
    "# Step 2: Compute max length\n",
    "max_len = max(len(seq) for seq in input_ids_list)\n",
    "\n",
    "# Step 3: Pad all sequences\n",
    "def pad(seq, pad_val, max_len):\n",
    "    return seq + [pad_val] * (max_len - len(seq))\n",
    "\n",
    "input_ids = torch.tensor([pad(seq, pad_val=1, max_len=max_len) for seq in input_ids_list])           # pad_val=1 for RoBERTa\n",
    "attention_masks = torch.tensor([pad(seq, pad_val=0, max_len=max_len) for seq in attention_masks_list])\n",
    "labels = torch.tensor([pad(seq, pad_val=-100, max_len=max_len) for seq in label_ids_list])           # -100 for ignored loss\n",
    "\n",
    "# Step 4: Wrap into TensorDataset\n",
    "test_dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(label_to_id),\n",
    "    id2label={v: k for k, v in label_to_id.items()},\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:10<00:00,  1.29it/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5971\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=0.137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3016\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.36it/s, loss=0.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.2180\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s, loss=0.0905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.1595\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.34it/s, loss=0.0834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.1261\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s, loss=0.0404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0898\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.32it/s, loss=0.0572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0585\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=0.0633] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0455\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.39it/s, loss=0.0652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0346\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s, loss=0.00716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss # batch loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_masks, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            true_seq = labels[i].cpu().numpy()\n",
    "            pred_seq = predictions[i].cpu().numpy()\n",
    "\n",
    "            for t, p in zip(true_seq, pred_seq):\n",
    "                if t != -100:\n",
    "                    true_labels.append(t)\n",
    "                    pred_labels.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64        19\n",
      "           1       0.48      0.43      0.45        23\n",
      "           2       0.97      0.96      0.96       409\n",
      "\n",
      "    accuracy                           0.92       451\n",
      "   macro avg       0.67      0.71      0.68       451\n",
      "weighted avg       0.93      0.92      0.92       451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    pred_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {v: k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, model, tokenizer, id2label, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input sentence, return PyTorch tensors and offset mappings\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    offset_mapping = encoding[\"offset_mapping\"][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # [1, seq_len, num_labels]\n",
    "        predictions = torch.argmax(logits, dim=-1)[0]  # seq_len\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    labels = [id2label[pred.item()] for pred in predictions]\n",
    "\n",
    "    # Combine tokens and labels, ignoring special tokens (like CLS, SEP)\n",
    "    result = []\n",
    "    for token, label, (start, end) in zip(tokens, labels, offset_mapping):\n",
    "        if start == 0 and end == 0:\n",
    "            # Skip special tokens with offset (0,0)\n",
    "            continue\n",
    "        token_text = sentence[start:end]\n",
    "        result.append((token_text, label))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Old', 'B-SocialGroup'),\n",
       " ('sick', 'I-SocialGroup'),\n",
       " ('people', 'I-SocialGroup'),\n",
       " ('feel', 'O'),\n",
       " ('left', 'O'),\n",
       " ('behind', 'O'),\n",
       " ('which', 'O'),\n",
       " ('is', 'O'),\n",
       " ('also', 'O'),\n",
       " ('true', 'O'),\n",
       " ('for', 'O'),\n",
       " ('young', 'B-SocialGroup'),\n",
       " ('mothers', 'I-SocialGroup'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Old sick people feel left behind which is also true for young mothers.\", model=model, tokenizer=tokenizer, id2label=id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
